import gym
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.autograd import Variable

STATE_DIM = 80 * 80
HIDDEN1 = 200
HIDDEN2 = 3

class NNEstimator(nn.Module):

    def __init__(self):
        super(NNEstimator, self).__init__()
        self.conv1 = nn.Conv2d(1, 1, 5, 2)
        self.fc1 = nn.Linear(19 * 19, 50)
        self.fc2 = nn.Linear(50, 3)
        self.softmax = nn.Softmax(dim=1)
        # self.fc1 = nn.Linear(80*80, 3)
        self.prev_x = torch.zeros(80, 80)

    def forward(self, x):
        delta = x.data - self.prev_x
        print(delta.norm())
        
        self.prev_x = x.data
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = x.view(-1, 19 * 19)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # x = x.view(1, -1)
        # x = F.relu(self.fc1(x))
        # x = self.softmax(x)
        return x
        

class VanillaPolicyGradientAgent:

    def __init__(self):
        # Construction of NN
        self.nn = NNEstimator()
        pass

    # Given state, function returns probabilities for each of 3 actions
    def get_action_probs(self, state):
        pass

    def get_grad_log_probs(self, state):
        pass

    # Given state, function returns tuple (action, grad_log_probability) for that action
    def get_action(self, state):
        print(state.shape)

        state = np.expand_dims(state, axis=0)
        state = np.expand_dims(state, axis=0)

        tstate = torch.from_numpy(state)
        tstate = tstate.type(torch.FloatTensor)
        
        probs = self.nn.forward(Variable(tstate))
        topv, topi = torch.topk(probs, 1, dim=1)

        action = topi.data[0][0]
        action_prob = topv.data[0][0]

        print(action, action_prob)
        return action


def downsample(x):
    x = x[35:195]
    x = x[::2, ::2, 0]
    x[x == 109] = 0
    x[x == 144] = 0
    x[x != 0] = 1
    return x

def get_real_action(action):
    if action == 1:
        return 2
    elif action == 2:
        return 3
    else:
        return 0

def main():
    env = gym.make('Pong-v0')

    vpg = VanillaPolicyGradientAgent()

    observation = env.reset()

    observation = downsample(observation)
    prev_observation = np.zeros_like(observation)

    for i in range(1):
        total_reward = 0

        

        while True:
            env.render()

            #action = env.action_space.sample()
            action = vpg.get_action(observation - prev_observation)

            prev_observation = observation
            observation, reward, done, info = env.step(get_real_action(action))
            observation = downsample(observation)
            total_reward += reward

            if done:
                break

        observation = env.reset()

        observation = downsample(observation)
        prev_observation = np.zeros_like(observation)


if __name__ == '__main__':
    main()
